{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6a657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN...\n",
      "Training PPO...\n",
      "Training A2C...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp and xp are not of the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 214\u001b[39m\n\u001b[32m    211\u001b[39m a2c_steps, a2c_rewards = train_a2c(env_a2c, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, config)\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Plot\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[43muniform_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdqn_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2c_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2c_rewards\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36muniform_plot\u001b[39m\u001b[34m(dqn_steps, dqn_rewards, ppo_steps, ppo_rewards, a2c_steps, a2c_rewards)\u001b[39m\n\u001b[32m    174\u001b[39m dqn_steps_trimmed = dqn_steps[-\u001b[38;5;28mlen\u001b[39m(dqn_smoothed):]\n\u001b[32m    175\u001b[39m dqn_y = np.interp(common_x, dqn_steps_trimmed, dqn_smoothed)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m ppo_y = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommon_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppo_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_rewards\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m a2c_y = np.interp(common_x, a2c_steps, smooth(a2c_rewards))\n\u001b[32m    180\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:1642\u001b[39m, in \u001b[36minterp\u001b[39m\u001b[34m(x, xp, fp, left, right, period)\u001b[39m\n\u001b[32m   1639\u001b[39m     xp = np.concatenate((xp[-\u001b[32m1\u001b[39m:]-period, xp, xp[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m]+period))\n\u001b[32m   1640\u001b[39m     fp = np.concatenate((fp[-\u001b[32m1\u001b[39m:], fp, fp[\u001b[32m0\u001b[39m:\u001b[32m1\u001b[39m]))\n\u001b[32m-> \u001b[39m\u001b[32m1642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: fp and xp are not of the same length."
     ]
    }
   ],
   "source": [
    "# ðŸ§  RL Algorithm Comparison: DQN vs PPO vs A2C\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Environment setup\n",
    "class SnakeEnv:\n",
    "    def __init__(self, width=10, height=10, block_size=20):\n",
    "        pygame.init()\n",
    "        self.width, self.height, self.block = width, height, block_size\n",
    "        self.display = pygame.Surface((self.width * self.block, self.height * self.block))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        self.direction = (1, 0)\n",
    "        self.snake = [(self.width//2, self.height//2)]\n",
    "        self._place_food()\n",
    "        self.done, self.score = False, 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _place_food(self):\n",
    "        while True:\n",
    "            self.food = (random.randrange(self.width), random.randrange(self.height))\n",
    "            if self.food not in self.snake:\n",
    "                break\n",
    "\n",
    "    def _get_obs(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        dir_x, dir_y = self.direction\n",
    "        food_x, food_y = self.food\n",
    "\n",
    "        def danger_at(offset):\n",
    "            dx, dy = offset\n",
    "            new_x, new_y = head_x + dx, head_y + dy\n",
    "            return int(\n",
    "                new_x < 0 or new_x >= self.width or\n",
    "                new_y < 0 or new_y >= self.height or\n",
    "                (new_x, new_y) in self.snake\n",
    "            )\n",
    "\n",
    "        left  = (-dir_y, dir_x)\n",
    "        right = (dir_y, -dir_x)\n",
    "        front = (dir_x, dir_y)\n",
    "\n",
    "        danger = [danger_at(front), danger_at(right), danger_at(left)]\n",
    "\n",
    "        food_dx = int(np.sign(food_x - head_x))\n",
    "        food_dy = int(np.sign(food_y - head_y))\n",
    "\n",
    "        dir_features = [int(dir_x == 1), int(dir_x == -1), int(dir_y == 1), int(dir_y == -1)]\n",
    "\n",
    "        return np.array(danger + dir_features + [food_dx, food_dy], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        dirs = [(-1,0),(0,1),(1,0),(0,-1)]\n",
    "        new_dir = dirs[action]\n",
    "        if (new_dir[0]==-self.direction[0] and new_dir[1]==-self.direction[1]):\n",
    "            new_dir = self.direction\n",
    "        self.direction = new_dir\n",
    "\n",
    "        head = (self.snake[0][0]+new_dir[0], self.snake[0][1]+new_dir[1])\n",
    "        if (not 0<=head[0]<self.width or not 0<=head[1]<self.height or head in self.snake):\n",
    "            self.done = True\n",
    "            return self._get_obs(), -10, True, {}\n",
    "\n",
    "        self.snake.insert(0, head)\n",
    "        if head == self.food:\n",
    "            reward, self.score = 10, self.score+1\n",
    "            self._place_food()\n",
    "        else:\n",
    "            reward = -0.1\n",
    "            self.snake.pop()\n",
    "\n",
    "        return self._get_obs(), reward, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        H_px = self.height * self.block\n",
    "        W_px = self.width  * self.block\n",
    "        frame = np.zeros((H_px, W_px, 3), dtype=np.uint8)\n",
    "\n",
    "        for x, y in self.snake:\n",
    "            y0, y1 = y*self.block, (y+1)*self.block\n",
    "            x0, x1 = x*self.block, (x+1)*self.block\n",
    "            frame[y0:y1, x0:x1] = [0,255,0]\n",
    "\n",
    "        fx, fy = self.food\n",
    "        y0, y1 = fy*self.block, (fy+1)*self.block\n",
    "        x0, x1 = fx*self.block, (fx+1)*self.block\n",
    "        frame[y0:y1, x0:x1] = [255,0,0]\n",
    "\n",
    "        return frame\n",
    "\n",
    "# ðŸ§± Models\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.policy = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), nn.Linear(128, action_dim), nn.Softmax(dim=-1))\n",
    "        self.value = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ðŸ› ï¸ Placeholder Training Loops\n",
    "\n",
    "def train_dqn(env, policy_net, target_net, memory, optimizer, config):\n",
    "    episode_rewards, env_steps_list = [], []\n",
    "    total_env_steps = 0\n",
    "    for ep in range(config[\"num_episodes\"]):\n",
    "        state = env.reset()\n",
    "        total_reward, done = 0, False\n",
    "        while not done:\n",
    "            action = np.random.randint(4)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            total_env_steps += 1\n",
    "        episode_rewards.append(total_reward)\n",
    "        env_steps_list.append(total_env_steps)\n",
    "    return env_steps_list, episode_rewards\n",
    "\n",
    "def train_ppo(env, model, optimizer, config):\n",
    "    total_env_steps, env_steps_list, rewards = 0, [], []\n",
    "    for i in range(config[\"num_updates\"]):\n",
    "        total_env_steps += config[\"update_steps\"]\n",
    "        avg_reward = np.random.randint(0, 100)\n",
    "        rewards.append(avg_reward)\n",
    "        env_steps_list.append(total_env_steps)\n",
    "    return env_steps_list, rewards\n",
    "\n",
    "def train_a2c(env, model, optimizer, config):\n",
    "    total_env_steps, env_steps_list, rewards = 0, [], []\n",
    "    for ep in range(config[\"num_episodes\"]):\n",
    "        state = env.reset()\n",
    "        total_reward, done = 0, False\n",
    "        while not done:\n",
    "            action = np.random.randint(4)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            total_env_steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        env_steps_list.append(total_env_steps)\n",
    "    return env_steps_list, rewards\n",
    "\n",
    "# ðŸ“Š Final Plot\n",
    "\n",
    "def smooth(y, window=10):\n",
    "    return np.convolve(y, np.ones(window)/window, mode='valid')\n",
    "\n",
    "def uniform_plot(dqn_steps, dqn_rewards, ppo_steps, ppo_rewards, a2c_steps, a2c_rewards):\n",
    "    max_step = max(dqn_steps[-1], ppo_steps[-1], a2c_steps[-1])\n",
    "    common_x = np.linspace(0, max_step, 300)\n",
    "\n",
    "    def prep_interp(steps, rewards):\n",
    "        smoothed = smooth(rewards)\n",
    "        steps_trimmed = steps[-len(smoothed):]\n",
    "        return steps_trimmed, smoothed\n",
    "\n",
    "    dqn_x, dqn_y = prep_interp(dqn_steps, dqn_rewards)\n",
    "    ppo_x, ppo_y = prep_interp(ppo_steps, ppo_rewards)\n",
    "    a2c_x, a2c_y = prep_interp(a2c_steps, a2c_rewards)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(common_x, np.interp(common_x, dqn_x, dqn_y), label=\"DQN\")\n",
    "    plt.plot(common_x, np.interp(common_x, ppo_x, ppo_y), label=\"PPO\")\n",
    "    plt.plot(common_x, np.interp(common_x, a2c_x, a2c_y), label=\"A2C\")\n",
    "    plt.xlabel(\"Environment Steps\")\n",
    "    plt.ylabel(\"Smoothed Reward\")\n",
    "    plt.title(\"Fair Comparison of RL Algorithms\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ðŸš€ Launch Experiments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = {\"num_episodes\": 200, \"num_updates\": 200, \"update_steps\": 1024}\n",
    "\n",
    "env_dqn = SnakeEnv()\n",
    "env_ppo = SnakeEnv()\n",
    "env_a2c = SnakeEnv()\n",
    "\n",
    "state_dim = len(env_dqn._get_obs())\n",
    "action_dim = 4\n",
    "\n",
    "print(\"Training DQN...\")\n",
    "dqn_steps, dqn_rewards = train_dqn(env_dqn, None, None, None, None, config)\n",
    "\n",
    "print(\"Training PPO...\")\n",
    "ppo_steps, ppo_rewards = train_ppo(env_ppo, None, None, config)\n",
    "\n",
    "print(\"Training A2C...\")\n",
    "a2c_steps, a2c_rewards = train_a2c(env_a2c, None, None, config)\n",
    "\n",
    "uniform_plot(dqn_steps, dqn_rewards, ppo_steps, ppo_rewards, a2c_steps, a2c_rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
