{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pygame\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93754d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeEnv:\n",
    "    def __init__(self, width=10, height=10, block_size=20, vision = \"full\"):\n",
    "        pygame.init()\n",
    "        self.width, self.height, self.block = width, height, block_size\n",
    "        self.vision = vision  # \"full\" or \"limited\"\n",
    "        self.display = pygame.Surface((self.width * self.block,\n",
    "                                       self.height * self.block))\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "        self.direction = (1, 0)\n",
    "        self.snake = [(self.width//2, self.height//2)]\n",
    "        self._place_food()\n",
    "        self.done, self.score = False, 0\n",
    "        return self._get_obs()\n",
    "\n",
    "\n",
    "    def _place_food(self):\n",
    "        while True:\n",
    "            self.food = (random.randrange(self.width),\n",
    "                         random.randrange(self.height))\n",
    "            if self.food not in self.snake:\n",
    "                break\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.vision == \"limited\":\n",
    "            return self._get_limited_obs()\n",
    "        return self._get_full_obs()\n",
    "\n",
    "\n",
    "    def _get_full_obs(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        dir_x, dir_y = self.direction\n",
    "        food_x, food_y = self.food\n",
    "\n",
    "        def danger_at(offset):\n",
    "            dx, dy = offset\n",
    "            new_x, new_y = head_x + dx, head_y + dy\n",
    "            return int(\n",
    "                new_x < 0 or new_x >= self.width or\n",
    "                new_y < 0 or new_y >= self.height or\n",
    "                (new_x, new_y) in self.snake\n",
    "            )\n",
    "\n",
    "        left  = (-dir_y, dir_x)\n",
    "        right = (dir_y, -dir_x)\n",
    "        front = (dir_x, dir_y)\n",
    "\n",
    "        danger = [\n",
    "            danger_at(front),\n",
    "            danger_at(right),\n",
    "            danger_at(left)\n",
    "        ]\n",
    "\n",
    "        food_dx = int(np.sign(food_x - head_x))\n",
    "        food_dy = int(np.sign(food_y - head_y))\n",
    "\n",
    "        dir_features = [\n",
    "            int(dir_x == 1), int(dir_x == -1),\n",
    "            int(dir_y == 1), int(dir_y == -1)\n",
    "        ]\n",
    "\n",
    "        return np.array(danger + dir_features + [food_dx, food_dy], dtype=np.float32)\n",
    "    \n",
    "    def _get_limited_obs(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        obs_size = 5\n",
    "        radius = obs_size // 2\n",
    "\n",
    "        obs = np.zeros((obs_size, obs_size), dtype=np.float32)\n",
    "\n",
    "        for dy in range(-radius, radius + 1):\n",
    "            for dx in range(-radius, radius + 1):\n",
    "                x = head_x + dx\n",
    "                y = head_y + dy\n",
    "\n",
    "                if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "                    val = -1.0  # wall\n",
    "                elif (x, y) in self.snake:\n",
    "                    val = -0.5  # snake body\n",
    "                elif (x, y) == self.food:\n",
    "                    val = 1.0   # food\n",
    "                else:\n",
    "                    val = 0.0   # empty\n",
    "\n",
    "                obs[dy + radius][dx + radius] = val\n",
    "\n",
    "        return obs.flatten()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        dirs = [(-1,0),(0,1),(1,0),(0,-1)]\n",
    "        new_dir = dirs[action]\n",
    "        if (new_dir[0]==-self.direction[0] and\n",
    "            new_dir[1]==-self.direction[1]):\n",
    "            new_dir = self.direction\n",
    "        self.direction = new_dir\n",
    "\n",
    "        head = (self.snake[0][0]+new_dir[0],\n",
    "                self.snake[0][1]+new_dir[1])\n",
    "        if (not 0<=head[0]<self.width or\n",
    "            not 0<=head[1]<self.height or\n",
    "            head in self.snake):\n",
    "            self.done = True\n",
    "            return self._get_obs(), -10, True, {}\n",
    "\n",
    "        self.snake.insert(0, head)\n",
    "        if head == self.food:\n",
    "            reward, self.score = 10, self.score+1\n",
    "            self._place_food()\n",
    "        else:\n",
    "            reward = -0.1\n",
    "            self.snake.pop()\n",
    "\n",
    "        return self._get_obs(), reward, False, {}\n",
    "\n",
    "    # Inside your SnakeEnv class, overwrite render() with:\n",
    "\n",
    "    def render(self, delay=50, caption=\"Snake AI Viewer\"):\n",
    "        pygame.display.set_caption(caption)\n",
    "        win = pygame.display.set_mode((self.width * self.block, self.height * self.block))\n",
    "        win.fill((0, 0, 0))  # black background\n",
    "\n",
    "        # Draw snake\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(win, (0, 255, 0), pygame.Rect(x * self.block, y * self.block, self.block, self.block))\n",
    "\n",
    "        # Draw food\n",
    "        fx, fy = self.food\n",
    "        pygame.draw.rect(win, (255, 0, 0), pygame.Rect(fx * self.block, fy * self.block, self.block, self.block))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)\n",
    "        pygame.event.pump()\n",
    "        pygame.time.delay(delay)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, path):\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if os.path.exists(path):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Loaded checkpoint from {path}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling_average(rewards, window=50, label=\"RL\", title=\"Rolling Average Reward\"):\n",
    "    rolling_avg = [np.mean(rewards[max(0, i - window):i+1]) for i in range(len(rewards))]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rewards, label=\"Reward\")\n",
    "    plt.plot(rolling_avg, label=f\"Rolling Avg ({window})\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650896eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(model, env, mode=\"dqn\", n_episodes=10, max_steps=1000):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            total = 0\n",
    "            done = False\n",
    "            for _ in range(max_steps):\n",
    "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                if mode == \"dqn\":\n",
    "                    q_vals = model(state_tensor)\n",
    "                    action = q_vals.argmax(dim=1).item()\n",
    "                else:\n",
    "                    probs, _ = model(state_tensor)\n",
    "                    action = probs.argmax(dim=1).item()\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total += reward\n",
    "                if done:\n",
    "                    break\n",
    "            scores.append(total)\n",
    "    model.train()\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8893d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.value = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy(x), self.value(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24695763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dqn = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"eps_start\": 1.0,\n",
    "    \"eps_end\": 0.01,\n",
    "    \"eps_decay\": 5000,\n",
    "    \"target_update\": 10,\n",
    "    \"num_episodes\": 1000,\n",
    "    \"ckpt_path\": \"checkpoints/dqn_snake.pth\",\n",
    "    \"total_env_steps\": 300000\n",
    "}\n",
    "\n",
    "ppo_config = {\n",
    "    \"gamma\":        0.99,\n",
    "    \"gae_lambda\":   0.95,\n",
    "    \"clip_eps\":     0.2,\n",
    "    \"lr\":           2.5e-4,\n",
    "    \"epochs\":       4,\n",
    "    \"batch_size\":   64,\n",
    "    \"update_steps\": 1024,\n",
    "    \"num_updates\":  100,\n",
    "    \"total_env_steps\": 300000\n",
    "}\n",
    "\n",
    "\n",
    "config_a2c = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_episodes\": 1000,\n",
    "    \"total_env_steps\": 300000\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b91a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action_dqn(state, eps, model):\n",
    "    if random.random() < eps:\n",
    "        return random.randrange(4)\n",
    "    state_v = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_vals = model(state_v)\n",
    "    return q_vals.argmax(dim=1).item()\n",
    "\n",
    "def optimize_dqn_step(memory, policy_net, target_net, optimizer, config):\n",
    "    if len(memory) < config[\"batch_size\"]:\n",
    "        return\n",
    "    transitions = memory.sample(config[\"batch_size\"])\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    states      = torch.from_numpy(np.array(batch.state)).float().to(device)\n",
    "    actions     = torch.tensor(batch.action, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "    rewards     = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    next_states = torch.tensor(batch.next_state, dtype=torch.float32).to(device)\n",
    "    dones       = torch.tensor(batch.done, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "    next_q = target_net(next_states).max(1)[0].detach().unsqueeze(1)\n",
    "    expected_q = rewards + config[\"gamma\"] * next_q * (1 - dones)\n",
    "\n",
    "    loss = F.mse_loss(q_values, expected_q)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env, policy_net, target_net, memory, optimizer, config, checkpoint_path):\n",
    "    episode_rewards = []\n",
    "    env_steps_list = []\n",
    "    total_env_steps = 0\n",
    "    steps_done = 0\n",
    "\n",
    "    best_avg_reward = -float(\"inf\")  # ✅ NEW: Track best model\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)  # ✅ NEW: Make save folder\n",
    "\n",
    "    def get_epsilon(step):\n",
    "        return config[\"eps_end\"] + (config[\"eps_start\"] - config[\"eps_end\"]) * \\\n",
    "               np.exp(-1. * step / config[\"eps_decay\"])\n",
    "\n",
    "    while total_env_steps < config[\"total_env_steps\"]:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        ep_steps = 0\n",
    "\n",
    "        while True:\n",
    "            eps = get_epsilon(steps_done)\n",
    "            action = select_action_dqn(state, eps, policy_net)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            optimize_dqn_step(memory, policy_net, target_net, optimizer, config)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps_done += 1\n",
    "            ep_steps += 1\n",
    "            total_env_steps += 1\n",
    "\n",
    "            if done or total_env_steps >= config[\"total_env_steps\"]:\n",
    "                break\n",
    "\n",
    "        env_steps_list.append(total_env_steps)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        if len(episode_rewards) % config[\"target_update\"] == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if len(episode_rewards) % 50 == 0:\n",
    "            avg_50 = np.mean(episode_rewards[-50:])\n",
    "            print(f\"[DQN] Episode {len(episode_rewards)} | Avg(50): {avg_50:.2f}\")\n",
    "\n",
    "            # ✅ NEW: Save model if it's the best so far\n",
    "            if avg_50 > best_avg_reward:\n",
    "                best_avg_reward = avg_50\n",
    "                save_checkpoint(policy_net, optimizer, checkpoint_path)\n",
    "                print(f\"[DQN] New best model saved! Avg(50): {avg_50:.2f}\")\n",
    "\n",
    "    return env_steps_list, episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c00382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states, self.actions, self.log_probs = [], [], []\n",
    "        self.rewards, self.dones, self.values = [], [], []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma, lam):\n",
    "    advantages, gae = [], 0\n",
    "    values = values + [0]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    return advantages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env, model, buffer, steps, max_steps=1000):\n",
    "    state = env.reset()\n",
    "    actual_steps = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            probs, value = model(state_tensor)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        buffer.states.append(state)\n",
    "        buffer.actions.append(action.item())\n",
    "        buffer.log_probs.append(dist.log_prob(action).item())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.dones.append(done)\n",
    "        buffer.values.append(value.item())\n",
    "\n",
    "        state = next_state\n",
    "        actual_steps += 1\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "    return actual_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, buffer, config):\n",
    "    states = torch.tensor(buffer.states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(buffer.actions).to(device)\n",
    "    old_log_probs = torch.tensor(buffer.log_probs, dtype=torch.float32).to(device)\n",
    "    rewards, dones, values = buffer.rewards, buffer.dones, buffer.values\n",
    "\n",
    "    advantages = compute_gae(rewards, values, dones, config[\"gamma\"], config[\"gae_lambda\"])\n",
    "    returns = torch.tensor([a + v for a, v in zip(advantages, values)], dtype=torch.float32).to(device)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32).to(device)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    for _ in range(config[\"epochs\"]):\n",
    "        for i in range(0, len(states), config[\"batch_size\"]):\n",
    "            s_batch = states[i:i+config[\"batch_size\"]]\n",
    "            a_batch = actions[i:i+config[\"batch_size\"]]\n",
    "            r_batch = returns[i:i+config[\"batch_size\"]]\n",
    "            adv_batch = advantages[i:i+config[\"batch_size\"]]\n",
    "            logp_old_batch = old_log_probs[i:i+config[\"batch_size\"]]\n",
    "\n",
    "            probs, values = model(s_batch)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            logp = dist.log_prob(a_batch)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratio = torch.exp(logp - logp_old_batch)\n",
    "            clipped = torch.clamp(ratio, 1 - config[\"clip_eps\"], 1 + config[\"clip_eps\"]) * adv_batch\n",
    "            loss_clip = -torch.min(ratio * adv_batch, clipped).mean()\n",
    "            loss_value = F.mse_loss(values.squeeze(), r_batch)\n",
    "            loss = loss_clip + 0.5 * loss_value - 0.01 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a5d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, model, optimizer, config, checkpoint_path):\n",
    "    buffer = RolloutBuffer()\n",
    "    reward_history = []\n",
    "    env_steps_list = []\n",
    "    total_env_steps = 0\n",
    "\n",
    "    best_avg_reward = -float(\"inf\")  # ✅ Track best avg reward\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)  # ✅ Ensure save folder exists\n",
    "\n",
    "    while total_env_steps < config[\"total_env_steps\"]:\n",
    "        steps_collected = collect_trajectory(env, model, buffer, config[\"update_steps\"])\n",
    "        ppo_update(model, optimizer, buffer, config)\n",
    "        buffer.clear()\n",
    "\n",
    "        total_env_steps += steps_collected\n",
    "        eval_scores = evaluate_policy(model, env, mode=\"ppo\", n_episodes=10)\n",
    "        avg_reward = np.mean(eval_scores)\n",
    "        reward_history.append(avg_reward)\n",
    "        env_steps_list.append(total_env_steps)\n",
    "\n",
    "        print(f\"[PPO] Env Steps {total_env_steps} | Avg Eval Reward: {avg_reward:.2f}\")\n",
    "\n",
    "        # ✅ Save best model\n",
    "        if avg_reward > best_avg_reward:\n",
    "            best_avg_reward = avg_reward\n",
    "            save_checkpoint(model, optimizer, checkpoint_path)\n",
    "            print(f\"[PPO] New best model saved! Avg Eval Reward: {avg_reward:.2f}\")\n",
    "\n",
    "    return env_steps_list, reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c(env, model, optimizer, config, checkpoint_path):\n",
    "    rewards = []\n",
    "    env_steps_list = []\n",
    "    total_env_steps = 0\n",
    "    gamma = config[\"gamma\"]\n",
    "\n",
    "    best_avg_reward = -float(\"inf\")  # ✅ Track best average\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)  # ✅ Ensure folder exists\n",
    "\n",
    "    while total_env_steps < config[\"total_env_steps\"]:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        ep_steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            probs, value = model(state_tensor)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n",
    "            _, next_value = model(next_state_tensor)\n",
    "\n",
    "            target = reward + (1 - done) * gamma * next_value.item()\n",
    "            advantage = target - value.item()\n",
    "\n",
    "            policy_loss = -log_prob * advantage\n",
    "            value_loss = F.mse_loss(value, torch.tensor([[target]], device=device))\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            ep_steps += 1\n",
    "            total_env_steps += 1\n",
    "\n",
    "            if total_env_steps >= config[\"total_env_steps\"]:\n",
    "                break\n",
    "\n",
    "        env_steps_list.append(total_env_steps)\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # ✅ Print and save model every 10 episodes\n",
    "        if len(rewards) % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            print(f\"[A2C] Episode {len(rewards)} | Avg(10): {avg_reward:.2f}\")\n",
    "\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                save_checkpoint(model, optimizer, checkpoint_path)\n",
    "                print(f\"[A2C] New best model saved! Avg(10): {avg_reward:.2f}\")\n",
    "\n",
    "    return env_steps_list, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_results(path, steps, rewards):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump((steps, rewards), f)\n",
    "\n",
    "def load_results(path):\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "def smooth(y, window=20):\n",
    "    return [np.mean(y[max(0, i - window):i+1]) for i in range(len(y))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e35afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "print(\"=== Grid Search: DQN (Full + Limited) ===\")\n",
    "dqn_results = {}\n",
    "dqn_limited_results = {}\n",
    "\n",
    "lr_vals = [1e-3, 1e-4]\n",
    "gamma_vals = [0.99, 0.95]\n",
    "\n",
    "for vision in [\"full\", \"limited\"]:\n",
    "    is_limited = vision == \"limited\"\n",
    "    result_dict = dqn_limited_results if is_limited else dqn_results\n",
    "\n",
    "    for lr, gamma in product(lr_vals, gamma_vals):\n",
    "        tag = f\"lr: {lr:.0e} | gamma: {gamma}\"\n",
    "        file_tag = tag.replace(\":\", \"\").replace(\"|\", \"_\").replace(\" \", \"\")\n",
    "        if is_limited:\n",
    "            file_tag += \"_limited\"\n",
    "\n",
    "        result_path = f\"results/dqn_{file_tag}.pkl\"\n",
    "\n",
    "        cached = load_results(result_path)\n",
    "\n",
    "        if cached:\n",
    "            steps, rewards = cached\n",
    "            print(f\"Loaded: {tag} ({vision})\")\n",
    "        else:\n",
    "            print(f\"Training: {tag} ({vision})\")\n",
    "            env = SnakeEnv(vision=vision)\n",
    "            state_dim = len(env.reset())\n",
    "            action_dim = 4\n",
    "\n",
    "            policy_net = DQN(state_dim, action_dim).to(device)\n",
    "            target_net = DQN(state_dim, action_dim).to(device)\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "            memory = ReplayMemory(capacity=10000)\n",
    "\n",
    "            config = config_dqn.copy()\n",
    "            config[\"gamma\"] = gamma\n",
    "            config[\"total_env_steps\"] = 300_000\n",
    "\n",
    "            checkpoint_path = f\"checkpoints/dqn_{file_tag}.pth\"\n",
    "            steps, rewards = train_dqn(env, policy_net, target_net, memory, optimizer, config, checkpoint_path)\n",
    "\n",
    "            save_results(result_path, steps, rewards)\n",
    "\n",
    "        result_dict[tag] = (steps, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15dc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Full vision\n",
    "axs[0].set_title(\"DQN – Full Vision\")\n",
    "for tag, (steps, rewards) in dqn_results.items():\n",
    "    axs[0].plot(steps, smooth(rewards), label=tag)\n",
    "axs[0].set_xlabel(\"Environment Steps\")\n",
    "axs[0].set_ylabel(\"Total Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Limited vision\n",
    "axs[1].set_title(\"DQN – Limited Vision\")\n",
    "for tag, (steps, rewards) in dqn_limited_results.items():\n",
    "    axs[1].plot(steps, smooth(rewards), label=tag)\n",
    "axs[1].set_xlabel(\"Environment Steps\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "print(\"=== Grid Search: PPO (Full + Limited) ===\")\n",
    "ppo_results = {}\n",
    "ppo_limited_results = {}\n",
    "\n",
    "lr_vals = [2.5e-4, 1e-4]\n",
    "gamma_vals = [0.99, 0.95]\n",
    "\n",
    "for vision in [\"full\", \"limited\"]:\n",
    "    is_limited = vision == \"limited\"\n",
    "    result_dict = ppo_limited_results if is_limited else ppo_results\n",
    "\n",
    "    for lr, gamma in product(lr_vals, gamma_vals):\n",
    "        tag = f\"lr: {lr:.0e} | gamma: {gamma}\"\n",
    "        file_tag = tag.replace(\":\", \"\").replace(\"|\", \"_\").replace(\" \", \"\")\n",
    "        if is_limited:\n",
    "            file_tag += \"_limited\"\n",
    "\n",
    "        result_path = f\"results/ppo_{file_tag}.pkl\"\n",
    "\n",
    "        cached = load_results(result_path)\n",
    "\n",
    "        if cached:\n",
    "            steps, rewards = cached\n",
    "            print(f\"Loaded: {tag} ({vision})\")\n",
    "        else:\n",
    "            print(f\"Training: {tag} ({vision})\")\n",
    "            env = SnakeEnv(vision=vision)\n",
    "            state_dim = len(env.reset())\n",
    "            action_dim = 4\n",
    "\n",
    "            model = ActorCritic(state_dim, action_dim).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            config = ppo_config.copy()\n",
    "            config[\"lr\"] = lr\n",
    "            config[\"gamma\"] = gamma\n",
    "            config[\"total_env_steps\"] = 300_000\n",
    "\n",
    "            checkpoint_path = f\"checkpoints/ppo_{file_tag}.pth\"\n",
    "            steps, rewards = train_ppo(env, model, optimizer, config, checkpoint_path)\n",
    "\n",
    "            save_results(result_path, steps, rewards)\n",
    "\n",
    "        result_dict[tag] = (steps, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Full vision\n",
    "axs[0].set_title(\"PPO – Full Vision\")\n",
    "for tag, (steps, rewards) in ppo_results.items():\n",
    "    axs[0].plot(steps, smooth(rewards), label=tag)\n",
    "axs[0].set_xlabel(\"Environment Steps\")\n",
    "axs[0].set_ylabel(\"Total Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Limited vision\n",
    "axs[1].set_title(\"PPO – Limited Vision\")\n",
    "for tag, (steps, rewards) in ppo_limited_results.items():\n",
    "    axs[1].plot(steps, smooth(rewards), label=tag)\n",
    "axs[1].set_xlabel(\"Environment Steps\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a099785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "print(\"=== Grid Search: A2C (Full + Limited) ===\")\n",
    "a2c_results = {}\n",
    "a2c_limited_results = {}\n",
    "\n",
    "lr_vals = [1e-3, 1e-4]\n",
    "gamma_vals = [0.99, 0.95]\n",
    "\n",
    "for vision in [\"full\", \"limited\"]:\n",
    "    is_limited = vision == \"limited\"\n",
    "    result_dict = a2c_limited_results if is_limited else a2c_results\n",
    "\n",
    "    for lr, gamma in product(lr_vals, gamma_vals):\n",
    "        tag = f\"lr: {lr:.0e} | gamma: {gamma}\"\n",
    "        file_tag = tag.replace(\":\", \"\").replace(\"|\", \"_\").replace(\" \", \"\")\n",
    "        if is_limited:\n",
    "            file_tag += \"_limited\"\n",
    "\n",
    "        result_path = f\"results/a2c_{file_tag}.pkl\"\n",
    "\n",
    "        cached = load_results(result_path)\n",
    "\n",
    "        if cached:\n",
    "            steps, rewards = cached\n",
    "            print(f\"Loaded: {tag} ({vision})\")\n",
    "        else:\n",
    "            print(f\"Training: {tag} ({vision})\")\n",
    "            env = SnakeEnv(vision=vision)\n",
    "            state_dim = len(env.reset())\n",
    "            action_dim = 4\n",
    "\n",
    "            model = ActorCritic(state_dim, action_dim).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            config = config_a2c.copy()\n",
    "            config[\"lr\"] = lr\n",
    "            config[\"gamma\"] = gamma\n",
    "            config[\"total_env_steps\"] = 300_000\n",
    "\n",
    "            checkpoint_path = f\"checkpoints/a2c_{file_tag}.pth\"\n",
    "            steps, rewards = train_a2c(env, model, optimizer, config, checkpoint_path)\n",
    "\n",
    "            save_results(result_path, steps, rewards)\n",
    "\n",
    "        result_dict[tag] = (steps, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7161d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Full vision\n",
    "axs[0].set_title(\"A2C – Full Vision\")\n",
    "for tag, (steps, rewards) in a2c_results.items():\n",
    "    axs[0].plot(steps, smooth(rewards), label=tag)\n",
    "axs[0].set_xlabel(\"Environment Steps\")\n",
    "axs[0].set_ylabel(\"Total Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Limited vision\n",
    "axs[1].set_title(\"A2C – Limited Vision\")\n",
    "for tag, (steps, rewards) in a2c_limited_results.items():\n",
    "    axs[1].plot(steps, smooth(rewards), label=tag)\n",
    "axs[1].set_xlabel(\"Environment Steps\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# === Full Vision ===\n",
    "axs[0].set_title(\"Best RL Agents – Full Vision\")\n",
    "best_dqn = dqn_results[\"lr: 1e-03 | gamma: 0.95\"]\n",
    "best_ppo = ppo_results[\"lr: 3e-04 | gamma: 0.95\"]\n",
    "best_a2c = a2c_results[\"lr: 1e-03 | gamma: 0.99\"]\n",
    "\n",
    "axs[0].plot(best_dqn[0], smooth(best_dqn[1]), label=\"DQN (full)\")\n",
    "axs[0].plot(best_ppo[0], smooth(best_ppo[1]), label=\"PPO (full)\")\n",
    "axs[0].plot(best_a2c[0], smooth(best_a2c[1]), label=\"A2C (full)\")\n",
    "axs[0].set_xlabel(\"Environment Steps\")\n",
    "axs[0].set_ylabel(\"Total Reward\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# === Limited Vision ===\n",
    "axs[1].set_title(\"Best RL Agents – Limited Vision\")\n",
    "best_dqn_limited = dqn_limited_results[\"lr: 1e-04 | gamma: 0.95\"]\n",
    "best_ppo_limited = ppo_limited_results[\"lr: 3e-04 | gamma: 0.95\"]\n",
    "best_a2c_limited = a2c_limited_results[\"lr: 1e-04 | gamma: 0.99\"]\n",
    "\n",
    "axs[1].plot(best_dqn_limited[0], smooth(best_dqn_limited[1]), label=\"DQN (limited)\")\n",
    "axs[1].plot(best_ppo_limited[0], smooth(best_ppo_limited[1]), label=\"PPO (limited)\")\n",
    "axs[1].plot(best_a2c_limited[0], smooth(best_a2c_limited[1]), label=\"A2C (limited)\")\n",
    "axs[1].set_xlabel(\"Environment Steps\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10989718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_path(result_dict, algo_name, vision, is_limited=False):\n",
    "    best_score = -float(\"inf\")\n",
    "    best_file_tag = None\n",
    "\n",
    "    for tag, (steps, rewards) in result_dict.items():\n",
    "        avg = np.mean(rewards[-10:])\n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            file_tag = tag.replace(\":\", \"\").replace(\"|\", \"_\").replace(\" \", \"\")\n",
    "            if is_limited:\n",
    "                file_tag += \"_limited\"\n",
    "            best_file_tag = file_tag\n",
    "\n",
    "    checkpoint_path = f\"checkpoints/{algo_name}_{best_file_tag}.pth\"\n",
    "    return checkpoint_path, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "dqn_full_path, dqn_full_score = get_best_model_path(dqn_results, \"dqn\", \"full\", is_limited=False)\n",
    "dqn_lim_path, dqn_lim_score = get_best_model_path(dqn_limited_results, \"dqn\", \"limited\", is_limited=True)\n",
    "\n",
    "# PPO\n",
    "ppo_full_path, ppo_full_score = get_best_model_path(ppo_results, \"ppo\", \"full\", is_limited=False)\n",
    "ppo_lim_path, ppo_lim_score = get_best_model_path(ppo_limited_results, \"ppo\", \"limited\", is_limited=True)\n",
    "\n",
    "# A2C\n",
    "a2c_full_path, a2c_full_score = get_best_model_path(a2c_results, \"a2c\", \"full\", is_limited=False)\n",
    "a2c_lim_path, a2c_lim_score = get_best_model_path(a2c_limited_results, \"a2c\", \"limited\", is_limited=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0957c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEST MODELS BELOW\")\n",
    "print(f\"DQN Full: {dqn_full_path} | Score: {dqn_full_score:.2f}\")\n",
    "print(f\"DQN Limited: {dqn_lim_path} | Score: {dqn_lim_score:.2f}\")\n",
    "print(f\"PPO Full: {ppo_full_path} | Score: {ppo_full_score:.2f}\")\n",
    "print(f\"PPO Limited: {ppo_lim_path} | Score: {ppo_lim_score:.2f}\")\n",
    "print(f\"A2C Full: {a2c_full_path} | Score: {a2c_full_score:.2f}\")\n",
    "print(f\"A2C Limited: {a2c_lim_path} | Score: {a2c_lim_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_model(env, model_path, mode=\"dqn\", max_steps=1000):\n",
    "    from time import sleep\n",
    "\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = 4  # Up, down, left, right\n",
    "\n",
    "    # Load correct model\n",
    "    if mode == \"dqn\":\n",
    "        model = DQN(state_dim, action_dim).to(device)\n",
    "    else:\n",
    "        model = ActorCritic(state_dim, action_dim).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())  # dummy optimizer for loading\n",
    "    load_checkpoint(model, optimizer, model_path)\n",
    "    model.eval()\n",
    "\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        env.render(caption=f\"Now Playing: {mode.upper()} ({env.vision})\")\n",
    "        sleep(0.05)\n",
    "\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        if mode == \"dqn\":\n",
    "            action = model(state_tensor).argmax(dim=1).item()\n",
    "        else:\n",
    "            probs, _ = model(state_tensor)\n",
    "            action = probs.argmax(dim=1).item()\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from functools import partial\n",
    "\n",
    "def launch_viewer(model_path, mode, vision):\n",
    "    env = SnakeEnv(vision=vision)\n",
    "    replay_model(env, model_path, mode=mode)\n",
    "\n",
    "def launch_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Snake AI Model Viewer\")\n",
    "\n",
    "    tk.Label(root, text=\"Select a model to view:\", font=(\"Arial\", 14)).pack(pady=10)\n",
    "\n",
    "    model_buttons = [\n",
    "        (\"DQN (Full)\", dqn_full_path, \"dqn\", \"full\"),\n",
    "        (\"DQN (Limited)\", dqn_lim_path, \"dqn\", \"limited\"),\n",
    "        (\"PPO (Full)\", ppo_full_path, \"ppo\", \"full\"),\n",
    "        (\"PPO (Limited)\", ppo_lim_path, \"ppo\", \"limited\"),\n",
    "        (\"A2C (Full)\", a2c_full_path, \"a2c\", \"full\"),\n",
    "        (\"A2C (Limited)\", a2c_lim_path, \"a2c\", \"limited\"),\n",
    "    ]\n",
    "\n",
    "    for name, path, mode, vision in model_buttons:\n",
    "        btn = tk.Button(root, text=name, width=30, font=(\"Arial\", 12),\n",
    "                        command=partial(launch_viewer, path, mode, vision))\n",
    "        btn.pack(pady=5)\n",
    "\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d55ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch_gui()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
